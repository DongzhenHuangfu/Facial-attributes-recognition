{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Attributes Recognization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This project uses the data from CelebFaces Attributes (CelebA) Dataset on Kaggle, which was originally collected by researchers at MMLAB, The Chinese University of Hong Kong (specific reference in Acknowledgment section).\n",
    "\n",
    "#### The main job of this project is to build and train a deep learning network via Keras for recognizing the 40 different facial attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in the labels......\n",
      "Finish!\n"
     ]
    }
   ],
   "source": [
    "# define a function to translate the number which is type of string into type int\n",
    "def trans_str2int(strlist):\n",
    "    intlist = []\n",
    "    for i in range(len(strlist)):\n",
    "        intlist.append([])\n",
    "        for j in range(len(strlist[i])):\n",
    "            intlist[i].append(int(strlist[i][j]))\n",
    "    return np.array(intlist, dtype = np.int16)\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "facial_attributes = []\n",
    "\n",
    "print('Reading in the labels......')\n",
    "with open('./data/list_attr_celeba.csv') as csvfile:\n",
    "    lines = csv.reader(csvfile)\n",
    "    for line in lines:\n",
    "        del line[0]\n",
    "        Y.append(line)\n",
    "    facial_attributes = Y[0]\n",
    "    del Y[0]\n",
    "    Y = trans_str2int(Y)\n",
    "print('Finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in the pixel datas......\n",
      "Finish!\n"
     ]
    }
   ],
   "source": [
    "print('Reading in the pixel datas......')\n",
    "images = glob.glob('./data/img/*.jpg')\n",
    "\n",
    "for file in images:\n",
    "    image = mpimg.imread(file)\n",
    "    X.append(image)\n",
    "print('Finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in the suggestion for spliting data......\n",
      "Finish!\n"
     ]
    }
   ],
   "source": [
    "print('Reading in the suggestion for spliting data......')\n",
    "suggestion = []\n",
    "with open('./data/list_eval_partition.csv') as csvfile:\n",
    "    lines = csv.reader(csvfile)\n",
    "    for line in lines:\n",
    "        suggestion.append(line[1])\n",
    "    del suggestion[0]\n",
    "    suggestion = trans_str2int(suggestion)\n",
    "print('Finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of X is: 202599\n",
      "The length of Y is: 202599\n",
      "The length of suggestion is: 202599\n"
     ]
    }
   ],
   "source": [
    "print('The length of X is: %d' % len(X))\n",
    "print('The length of Y is: %d' % len(Y))\n",
    "print('The length of suggestion is: %d' % len(suggestion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliting the data......\n",
      "Finish!\n"
     ]
    }
   ],
   "source": [
    "print('Spliting the data......')\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_valid = []\n",
    "y_valid = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "for i in range(len(suggestion)):\n",
    "    if suggestion[i] == 0:\n",
    "        x_train.append(X[i])\n",
    "        y_train.append(Y[i])\n",
    "    elif suggestion[i] == 1:\n",
    "        x_valid.append(X[i])\n",
    "        y_valid.append(Y[i])\n",
    "    else:\n",
    "        x_test.append(X[i])\n",
    "        y_test.append(Y[i])\n",
    "print('Finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of training data is: 162770\n",
      "The length of validation data is: 19867\n",
      "The length of testing data is: 19962\n"
     ]
    }
   ],
   "source": [
    "print('The length of training data is: %d' % len(x_train))\n",
    "print('The length of validation data is: %d' % len(x_valid))\n",
    "print('The length of testing data is: %d' % len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X, Y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-9d3513bd3863>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dict1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mdata_dict1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## save the original dataï¼Œ release the memory\n",
    "with open('data_original_xtrain.pickle', 'wb') as file:\n",
    "    x_train = np.array(x_train, dtype = np.int16)\n",
    "    data_dict1 = {'x_train': x_train}\n",
    "    del x_train\n",
    "    gc.collect()\n",
    "    pickle.dump(data_dict1,file)\n",
    "del data_dict1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_original_others.pickle', 'wb') as file:\n",
    "    x_valid = np.array(x_valid, dtype = np.int16)\n",
    "    x_test = np.array(x_test, dtype = np.int16)\n",
    "    y_train = np.array(y_train, dtype = np.int8)\n",
    "    y_valid = np.array(y_valid, dtype = np.int8)\n",
    "    y_test = np.array(y_test, dtype = np.int8)\n",
    "    data_dict2 = {'y_train': y_train, 'x_valid': x_valid, 'y_valid': y_valid, \n",
    "                  'x_test': x_test, 'y_test': y_test, 'facial_attributes': facial_attributes}\n",
    "    del y_train, x_valid, y_valid, x_test, y_test, facial_attributes\n",
    "    gc.collect()\n",
    "    pickle.dump(data_dict2,file)   \n",
    "\n",
    "del y_train, x_valid, y_valid, x_test, y_test, facial_attributes, data_dict2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the data\n",
    "with  openopen('project_data.pickle', 'rb') as file:\n",
    "    data_dict =  pickle.load(file)\n",
    "    x_train = data_dict1['x_train']\n",
    "    y_train = data_dict2['y_train']\n",
    "    x_valid = data_dict2['x_valid']\n",
    "    y_valid = data_dict2['y_valid']\n",
    "    x_test = data_dict2['x_test']\n",
    "    y_test = data_dict2['y_test']\n",
    "    facial_attributes = data_dict2['facial_attributes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize data\n",
    "\n",
    "index = random.randint(0, len(x_train))\n",
    "image = x_train[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image, cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize the data, cause there is a memory error with x_train, so I define the datatype as float16 to save the memory\n",
    "x_train = (np.array(x_train, dtype = np.float16) - 128.)/256.\n",
    "x_valid = (np.array(x_valid, dtype = np.float16) - 128.)/256.\n",
    "x_test = (np.array(x_test, dtype = np.float16) - 128.)/256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the data\n",
    "data_dict = {'x_train': x_train, 'y_train': y_train, 'x_valid': x_valid, \n",
    "             'y_valid': y_valid, 'x_test': X_test, 'y_test': y_test}\n",
    "\n",
    "with open('project_data.pickle', 'wb') as file:\n",
    "    pickle.dump(data_dict,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the data\n",
    "\n",
    "with  openopen('project_data.pickle', 'rb') as file:\n",
    "    data_dict =  pickle.load(file)\n",
    "    x_train = data_dict['x_train']\n",
    "    y_train = data_dict['y_train']\n",
    "    x_valid = data_dict['x_valid']\n",
    "    y_valid = data_dict['y_valid']\n",
    "    x_test = data_dict['x_test']\n",
    "    facial_attributes = data_dict['facial_attributes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Keras Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoches = 30\n",
    "batch_size = 125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Build Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
